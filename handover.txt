
GRIT handover:

What is GRIT?
GRIT [Generic Rules for Information Transformation] is a set of tools used to verify the accuracy of a database. Grit uses a set of rules to verify that every record in one table has a corresponding record in the other table and vice versa. So for example a rule such as in.x + in.y <=> out.z could be used to confirm that for every record in table 'in', the sum of the attributes 'x' and 'y' are recorded in table 'out' and that every record in table 'out' has a matching record in table 'in'. 

Grit also includes the tools required to process, parse and load the node data to produce the table that the rules can be verified against, and to clean it up afterwards. As such, it operates in five modes called actions:
-a 10dash 	= parse a 10dash document to produce a schema
-a raw 	= parse raw data using schema to produce SQL insert statements
-a sql	= execute the SQL in the named file to load data into tables
-a rules 	= execute the rules in specified file  
-a cleanup	= remove temporary tables from db.
  
The Confluence page for Grit is here: 
http://confluence-nam.lmera.ericsson.se/display/TO/GRIT+-+Generic+Rules+for+Information+Transformation

=============================
Grit is written in Jython 2.7 (a version of Python implemented in Java instead of C) which generates and executes SQL statements (via JDBC) to implement and verify transformations described by the rules and to identify the differences between tables. 

It is distributed as a command line runnable jar using Java version 6 or greater. 
Normal execution is:
	# java -jar Grit.jar [options]

The startup Main() is in
  java-src/Main.Java
which feeds python-src/entrypoint.py into the python interpreter. 
python-src/entrypoint.py just imports and calls grit main(). 
This is managed by an 'ant' build script called build.xml 

Advantage: This allows java -jar xx.jar to execute jython programs directly so there is no need to install jython on target machines.  
Disadvantage: It is unable to return error codes to a calling script. If a rule fails, GRIT will exit with a runtime exception “Errors Detected”. This exception will also be thrown if GRIT finishs abnormally for any other reason.

Jython libraries are extracted into build.dir

Main method: grit.py main()
Multiple runtime modes controlled by the action CLI flag '-a'. 
Other options dependent on selected action. 
[OFI refactor to use argparse to link help text to supported options]
Debug mode can be triggered by using grit_db.py (note that it uses 'src' as its current working directory)

Release page is located at 
https://arm1s11-eiffel004.eiffel.gic.ericsson.se:8443/nexus/content/repositories/assure-releases/com/ericsson/eniq/events/Grit/

Build page is located at
https://fem101-eiffel013.lmera.ericsson.se:8443/jenkins/job/GRIT/

Development environment described here: 
http://confluence-nam.lmera.ericsson.se/display/TO/GRIT+-+Development

Confluence page is here: 
http://confluence-nam.lmera.ericsson.se/display/TO/GRIT+-+Generic+Rules+for+Information+Transformation

 

=============================
Version control: git
clone from: ssh://<signum>@gerrit.ericsson.se:29418/OSS/com.ericsson.eniq/GRIT <targetDir>
Version number held in two locations with manual synching. 
Version displayed by program is held in 'version' variable in grit.py. 
Version shown on release page gets major and minor version numbers from build.cfg and autoincrements build number when changes are pushed. Resetting when build.cfg is changed. This means that when making changes, remember to update version number in grit.py once for each push. 

============================
Bits vs. bytes
Note that because the raw data can be in bytes or in bits, 
all data will be handled in bits for which we use the public library 'bitString.py'. This library is included in the source directory for simplicity but it is not ours and should not be modified in any way. In particular, do not attempt to put an Ericsson copyright message in it. 

============================
Schema: This is a structure that describes the raw data. It is generated by parsing the 10dash document and is used to parse the raw data into SQL insert statements. The logic is in schema.py and it is called from parse10dash.py and parseRaw.py. 

A schema consists of a header followed by one or more events. Each event has one or more fields, each field has attributes. A Field can be part of a sequence (ie zero or more repeats), it can be optional and/or variable length and it can have  a validity bit. Also included in this module are the methods to convert data types from the 10Dash into SQL insert statements. [OFI the methods to convert actual values are in ParseRaw.py and it would make sense to bring them closer together. Perhaps a single module for all conversions?]
The Header structure includes feature specific information that can not be determined by parsing the 10Dash. 

===========================
Supported data types
GRIT supports CTRS, CTUM, GPEH and SGEH data. Example 10dash documents are in the 'etc' directory. CTRS data is also known as LTE data. 

===========================
Parse10Dash.py 
This creates a schema from a 10Dash document that can be used to parse raw data. It also creates a text file with the SQL statements that create the tables required to store any data parsed with this schema.
[OFI the class for structure and structureDictionary are pretty clumsy and could be refactored away]
[OFI the Parse10Dash module could be broken into smaller and simpler chunks]
This module also renames counter fields when necessary and manages the reserved word list as detailed on the confluence page. The list of SybaseIQ reserved words was extracted from : http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc38151.1510/html/iqrefbb/Alhakeywords.htm
   

===========================
ParseRaw.py
Parse raw data using specified schema to produce SQL insert statements.
Recent change - Use the insert statement as a key to a dictionary and append the values. Write the SQL to file if the size exceeds 32K. 
Massivly smaller outfile sizes [82M->12M] and massivly faster execution [111 minutes -> 3 minutes for 62k records]

===========================
cleanup.py
convert the text "create table xxx" into drop table xxx" from given sql file
and drops any temporary tables created by GRIT that may be lying around.


===========================
dbAccess.py
Set of database access methods. Limited test code available in dbAccessTest.py which can be executed as jython.Junit tests.

===========================
Overlap.py
This class contains methods used to workout the overlap in time between two tables in the database. This is required because an ENIQ table will typically have data outside the range of the data being tested so this code produced the SQL 'when' clause required to constrain the queries. 

===========================
Rules.
The rest of Grit is about handling rules. 

parser.py
Converts a piece of text into a set of tokens. unit tests in parserTest.py
[OFI parser only handles the top layer. It does not expand brackets etc. but this would require a significant re-think.]

Note that checkRule() only checks that a rule is syntactically valid, it does not check that it is grammatically correct, that is handled later.

Keyword.py 
Handles the parsing and processing of keywords. unit tests in keywordTest.py
[OFI could be cleaned up quite a bit]

grammer.py & generateSQL.py
Process a single rule to produce a list of tokens which are then used to populate all the bits that will later be used to generate some SQL. This code is muddled and would benefit from a serious refactoring. I think something like a new module that contains the model of a rule, produced by grammer.py and consumed by generateSQL.py. This could be incorporated into parser.py allowing it to go deeper.  



===========================
Configuration. 
GRIT uses a number of input files depending on the mode it is operating in. 

To parse a 10dash document, Grit requires the name of the document to parse, the name of the schema to write to and the name of the feature being processed. The feature must be known to determine information such as length and offset of the event ID ois required for hardcoded 


 
 
 

